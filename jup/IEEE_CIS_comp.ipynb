{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"import numpy as np \nimport pandas as pd \n\nimport lightgbm as lgb\nimport catboost\n\nfrom sklearn.metrics import roc_auc_score, log_loss\nfrom sklearn.model_selection import TimeSeriesSplit, train_test_split\nfrom sklearn.preprocessing import LabelEncoder\n\nfrom tqdm import tqdm\n\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport scipy.stats as sp\n\nimport warnings, random, gc, os,datetime, shap\nwarnings.simplefilter(action='ignore', category=FutureWarning)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def set_seed(seed=0):\n    random.seed(seed)\n    os.environ['PYTHONHASHSEED'] = str(seed)\n    np.random.seed(seed)\n    \nSEED = 4242\nset_seed(SEED)\n\nlgb_params = {\n    'objective':'cross_entropy',\n    'boosting_type':'gbdt',\n    'metric':'auc',\n    'n_jobs':-1,\n    'seed': SEED,\n    'is_unbalance': True,\n    \n    'learning_rate':0.01,\n\n    'num_leaves': 60,\n    'max_depth': 7,\n    'subsample_freq':1,\n    'subsample':0.7,\n    'colsample_bytree': 0.7\n    } ","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# load"},{"metadata":{"trusted":true},"cell_type":"code","source":"def get_df(df_path, dtypes_path):\n    path = '../input/ieee-cis-data/'\n    df_dtypes = pd.read_csv(path + dtypes_path)\n    df_dtypes = df_dtypes.rename(columns={'TransactionID': 'col','int32': 'type'})\n    df_dtypes = df_dtypes.to_dict(orient='records')\n    new_dtypes = dict()\n    for rec in df_dtypes:\n        new_dtypes[rec['col']] = rec['type']\n    new_dtypes['TransactionID'] = 'int32'\n\n    df = pd.read_csv(path+df_path, dtype=new_dtypes)\n    del df_dtypes, new_dtypes\n    gc.collect()\n    return df\n\ndef load_data():\n    train = get_df('train.csv', 'train_dtypes.csv')\n    test = get_df('test.csv', 'test_dtypes.csv')\n    sub = pd.read_csv('../input/ieee-fraud-detection/sample_submission.csv')\n    return train, test, sub\n\ndef get_x_y(train, test):\n    X = train.sort_values('TransactionDT').drop(['isFraud',  'TransactionID','TransactionDT'], axis=1)\n    y_train = train.sort_values('TransactionDT')['isFraud']\n    X_test = test.sort_values('TransactionDT').drop(['TransactionID','TransactionDT'], axis=1)\n    del train\n    gc.collect()\n    test = test[[\"TransactionDT\", 'TransactionID']]\n    return X,y_train,X_test, test\n\ndef reduce_memory_usage(df, cols=None):\n        numerics = ['int16', 'int32', 'int64', 'float16', 'float32', 'float64']\n        start_mem = df.memory_usage().sum() / 1024 ** 2\n        if cols is None:\n            cols = df.columns\n        for col in tqdm(cols):\n            col_type = df[col].dtypes\n            if col_type in numerics:\n                c_min = df[col].min()\n                c_max = df[col].max()\n                if str(col_type)[:3] == 'int':\n                    if c_min > np.iinfo(np.int8).min and c_max < np.iinfo(np.int8).max:\n                        df[col] = df[col].astype(np.int8)\n                    elif c_min > np.iinfo(np.int16).min and c_max < np.iinfo(np.int16).max:\n                        df[col] = df[col].astype(np.int16)\n                    elif c_min > np.iinfo(np.int32).min and c_max < np.iinfo(np.int32).max:\n                        df[col] = df[col].astype(np.int32)\n                    elif c_min > np.iinfo(np.int64).min and c_max < np.iinfo(np.int64).max:\n                        df[col] = df[col].astype(np.int64)\n                else:\n                    if c_min > np.finfo(np.float16).min and c_max < np.finfo(np.float16).max:\n                        df[col] = df[col].astype(np.float16)\n                    elif c_min > np.finfo(np.float32).min and c_max < np.finfo(np.float32).max:\n                        df[col] = df[col].astype(np.float32)\n                    else:\n                        df[col] = df[col].astype(np.float64)\n        end_mem = df.memory_usage().sum() / 1024 ** 2\n        print('Mem. usage decreased to {:5.2f} Mb ({:.1f}% reduction)'.format(end_mem, 100 * (\n                start_mem - end_mem) / start_mem))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# adv"},{"metadata":{"trusted":true},"cell_type":"code","source":"def get_adversarial_train(X, X_test,seed=SEED):\n    X[\"is_test\"] = 0\n    X_test[\"is_test\"] = 1\n    assert(np.all(X_train.columns == X_test.columns))\n    \n    print('Concat')\n    total = pd.concat([X, X_test])\n#     X.drop('is_test',1,inplace=True)\n#     X_test.drop('is_test',1,inplace=True)\n    del X, X_test\n    gc.collect()\n    \n    X_split = total.drop([\"is_test\"], axis = 1)\n    y_split = total.is_test\n    del total\n    gc.collect()\n    \n    print('Split')\n    dataX_train, dataX_valid, datay_train, datay_valid = train_test_split(X_split,y_split, test_size=0.2, random_state=seed)\n    del X_split,y_split\n    gc.collect()\n    \n    dtrain = lgb.Dataset(data=dataX_train, label=datay_train,free_raw_data=False)\n    del dataX_train, datay_train\n    gc.collect()\n    \n    dval = lgb.Dataset(data=dataX_valid, label=datay_valid,free_raw_data=False)\n    del dataX_valid, datay_valid\n    gc.collect()\n\n    print('Train')\n    clf = lgb.train(lgb_params, dtrain, \n                    num_boost_round=1000,\n                    verbose_eval=200,\n                    early_stopping_rounds=100, \n                    valid_sets = [dtrain, dval])\n    \n    feature_importance = pd.DataFrame()\n    feature_importance[\"feature\"] = X.columns\n    feature_importance[\"importance\"] = clf.feature_importance()\n\n    cols = feature_importance[[\"feature\", \"importance\"]].groupby(\"feature\").mean().sort_values(\"importance\", ascending=False)[:50].index\n\n    best_features = feature_importance.loc[feature_importance.feature.isin(cols)]\n    plt.figure(figsize=(16, 12))\n    sns.barplot(x=\"importance\",\n                    y=\"feature\",\n                    data=best_features.sort_values(by=\"importance\", ascending=False))\n    plt.title('LGB Features (avg over folds)')\n    \ndef adv(X, X_test):\n    a,b = get_adversarial_train(X,X_test)\n    del a,b\n    gc.collect()\n    X.drop(['is_test','target'],1,inplace=True)\n    X_test.drop(['is_test','target'],1,inplace=True)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# eval"},{"metadata":{"trusted":true},"cell_type":"code","source":"def eval_ho(X,y, X_test=None):\n    X_train = X.head(550000)\n    Y_train = y[X_train.index]\n    X_valid = X.tail(40540)\n    Y_valid = y[X_valid.index]\n    columns = list(X.columns)\n    \n    del X, y_train\n    gc.collect()\n    dtrain = lgb.Dataset(data=X_train, label=Y_train)\n    dval = lgb.Dataset(data=X_valid, label=Y_valid)\n    \n    clf = lgb.train(lgb_params, dtrain, \n                    num_boost_round=1000,\n                    verbose_eval=200,\n                    early_stopping_rounds=100, \n                    valid_sets = [dtrain, dval])\n    del dval, dtrain\n    gc.collect()\n    \n    oof = clf.predict(X_valid)\n    train_pred = clf.predict(X_train)\n    \n    print(f'TRAIN AUC: {roc_auc_score(Y_train, train_pred)}\\t TRAIN LOG: {log_loss(Y_train,  train_pred)}')\n    print(f'VALID AUC: {roc_auc_score(Y_valid, oof)}\\t VALID LOG: {log_loss(Y_valid,  oof)}')\n    del oof, train_pred\n    gc.collect()\n    \n    feature_importance = pd.DataFrame()\n    feature_importance[\"feature\"] = columns\n    feature_importance[\"importance\"] = clf.feature_importance()\n\n    cols = feature_importance[[\"feature\", \"importance\"]].groupby(\"feature\").mean().sort_values(\"importance\", ascending=False)[:50].index\n\n    best_features = feature_importance.loc[feature_importance.feature.isin(cols)]\n    plt.figure(figsize=(16, 12))\n    sns.barplot(x=\"importance\",\n                    y=\"feature\",\n                    data=best_features.sort_values(by=\"importance\", ascending=False))\n    plt.title('LGB Features (avg over folds)')\n    del feature_importance,best_features,cols\n    gc.collect()\n    \n    X= pd.concat([X_train, X_valid])\n    del X_train, X_valid\n    gc.collect()\n    y = np.concatenate([Y_train, Y_valid])\n    del Y_train, Y_valid\n    gc.collect()\n    \n    if X_test is not None:\n        pred = clf.predict(X_test)\n        return clf, pred, clf.current_iteration(), X,y\n    else: \n        return clf, clf.current_iteration(), X,y\n    \ndef eval_ts(X, y):\n    ts = TimeSeriesSplit(n_splits=5)\n    tr_aucs = []\n    tr_logs = []\n    val_aucs = []\n    val_logs = []\n    for i,(tr,val) in enumerate(ts.split(X,y)):\n        X_tr, X_val = X.iloc[tr], X.iloc[val]\n        y_tr, y_val = y[tr],y[val]\n\n        dtr = lgb.Dataset(data=X_tr, label=y_tr)\n        dvl = lgb.Dataset(data=X_val, label=y_val)\n\n        clf = lgb.train(lgb_params, dtr, \n                    num_boost_round=3000,\n                    verbose_eval=False,\n                    early_stopping_rounds=200, \n                    valid_sets = [dtr, dvl])\n\n        tr_pred = clf.predict(X_tr)\n        vl_pred = clf.predict(X_val)\n\n        score_auc_tr = roc_auc_score(y_tr, tr_pred)\n        score_log_tr = log_loss(y_tr,tr_pred)\n        score_auc_val = roc_auc_score(y_val, vl_pred)\n        score_log_val = log_loss(y_val,vl_pred)\n\n        tr_aucs.append(score_auc_tr)\n        tr_logs.append(score_log_tr)\n        val_aucs.append(score_auc_val)\n        val_logs.append(score_log_val)\n\n        print('FOLD {} TRAIN auc {:.7f} log {:.7f}\\t VALID auc {:.7f} log {:.7f}'.\n              format(i, score_auc_tr, score_log_tr ,score_auc_val, score_log_val))\n    feature_importance = pd.DataFrame()\n    feature_importance[\"feature\"] = X.columns\n    feature_importance[\"importance\"] = clf.feature_importance()\n\n    cols = feature_importance[[\"feature\", \"importance\"]].groupby(\"feature\").mean().sort_values(\"importance\", ascending=False)[:50].index\n\n    best_features = feature_importance.loc[feature_importance.feature.isin(cols)]\n    plt.figure(figsize=(16, 12))\n    sns.barplot(x=\"importance\",\n                    y=\"feature\",\n                    data=best_features.sort_values(by=\"importance\", ascending=False))\n    plt.title('LGB Features (avg over folds)')\n    print('Train AUC: mean', np.mean(tr_aucs), ', std:', np.std(tr_aucs))\n    print('Valid AUC: mean', np.mean(val_aucs),', std:', np.std(val_aucs))\n    print('Train LOG: mean',np.mean(tr_logs), ', std:',np.std(tr_logs))\n    print('Valid LOG: mean',np.mean(val_logs),', std:', np.std(val_logs))\n    del dtr, dvl\n    gc.collect()\n    return clf, clf.current_iteration()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# fe"},{"metadata":{"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","trusted":true},"cell_type":"code","source":"def fe_cat_other(train, test, obj_cols):\n    print('fe_cat_other')\n    for c in obj_cols:\n        tr_u = set(train[c].dropna().unique())\n        te_u = set(test[c].dropna().unique())\n        diff= {}\n        for u in tr_u:\n            if u not in te_u:\n                diff[u] = 'Other'\n        for u in te_u:\n            if u not in tr_u:\n                diff[u] = 'Other'\n        del tr_u, te_u\n        gc.collect()\n        train[c] = train[c].replace(diff)\n        test[c] = test[c].replace(diff)\n    del diff\n    gc.collect()\n\n    train.card6 = train.card6.replace({'charge card': 'Other'})\n    test.card6 = test.card6.replace({'charge card': 'Other'})\n\n    train.id_30 = train.id_30.replace({'func': 'Other','other':'Other'})\n    test.id_30 = test.id_30.replace({'func': 'Other','other':'Other'})\n\ndef fe_mail(train, test):\n    print('fe_mail')\n    train['P_isproton']=(train['P_emaildomain']=='protonmail.com')\n    train['R_isproton']=(train['R_emaildomain']=='protonmail.com')\n    test['P_isproton']=(test['P_emaildomain']=='protonmail.com')\n    test['R_isproton']=(test['R_emaildomain']=='protonmail.com')\n    \n    train[['P_emaildomain_1', 'P_emaildomain_2', 'P_emaildomain_3']] = train['P_emaildomain'].str.split('.', expand=True)\n    train[['R_emaildomain_1', 'R_emaildomain_2', 'R_emaildomain_3']] = train['R_emaildomain'].str.split('.', expand=True)\n    test[['P_emaildomain_1', 'P_emaildomain_2', 'P_emaildomain_3']] = test['P_emaildomain'].str.split('.', expand=True)\n    test[['R_emaildomain_1', 'R_emaildomain_2', 'R_emaildomain_3']] = test['R_emaildomain'].str.split('.', expand=True)\n\ndef fe_id_30(train, test):\n    print('fe_id_30')\n    os_map = {}\n    for i in train.id_30.dropna().unique():\n        os_map[i] = i.split(' ')[0]\n\n    for i in test.id_30.dropna().unique():\n        if i not in train.id_30.dropna().unique():\n            os_map[i] = 'Other'\n\n            os_map['func'] = 'Other'\n    os_map['other'] = 'Other'\n    train.id_30 = train.id_30.map(os_map)\n    test.id_30 = test.id_30.map(os_map)\n    del os_map\n    gc.collect()\n\ndef fe_date(train, test):\n    print('fe_date')\n    START_DATE = '2017-12-01'\n    startdate = datetime.datetime.strptime(START_DATE, \"%Y-%m-%d\")\n    \n    def extract_date(df):\n        df[\"Date\"] = df['TransactionDT'].apply(lambda x: (startdate + datetime.timedelta(seconds=x)))\n        df['Month'] = df['Date'].dt.month\n        df['Weekday'] = df['Date'].dt.dayofweek\n        df['Hour'] = df['Date'].dt.hour\n        df['Day'] = df['Date'].dt.day\n        df['DayOfYear'] = df['Date'].dt.dayofyear\n    extract_date(train)\n    extract_date(test)\n\ndef fe_le(train, test, obj_cols):\n    print('fe_le')\n    def do_le_nan(X, X_test, c):\n        mask_tr = X[c].isnull()\n        mask_te = X_test[c].isnull()\n        le = LabelEncoder()\n        le.fit(list(X[c].astype(str).values) + list(X_test[c].astype(str).values))\n        X[c] = le.transform(list(X[c].astype(str).values))\n        X_test[c] = le.transform(list(X_test[c].astype(str).values))\n        X[c] = X[[c]].where(~mask_tr)\n        X_test[c] = X_test[[c]].where(~mask_te)\n        del mask_tr, mask_te, le\n        gc.collect()\n    for c in obj_cols:\n        do_le_nan(train, test, c)\n    \n        \ndef fe_smooth(train, test, smooth_cols=None):\n    print('fe_smooth')\n    if smooth_cols is None:\n        smooth_cols = []\n        cols = list(train.columns)\n        cols.remove('isFraud')\n        for c in cols:\n            if train[c].nunique() != test[c].nunique():\n                smooth_cols.append(c)\n    def distr_smoothing(X, X_test, col):\n        agg_tr = X.groupby([col]).aggregate({col:'count'}).rename(columns={col:'Train'}).reset_index()\n        agg_te = X_test.groupby([col]).aggregate({col:'count'}).rename(columns={col:'Test'}).reset_index()\n        agg = pd.merge(agg_tr, agg_te, on=col,how='outer')\n\n        agg['Total'] = agg['Train'] + agg['Test']\n        agg = agg[(agg['Train'] / agg['Total'] > 0.2) & (agg['Train'] / agg['Total'] < 0.8)]\n        agg[col+'_Copy'] = agg[col]\n\n        X[col] = pd.merge(X[[col]], agg[[col,col+'_Copy']], on=col, how='left')[col+'_Copy']\n        X_test[col] = pd.merge(X_test[[col]], agg[[col,col+'_Copy']], on=col, how='left')[col+'_Copy']\n\n        del agg, agg_tr, agg_te\n        gc.collect()\n    for c in tqdm(smooth_cols):\n        distr_smoothing(train, test, c)\n    \n    reduce_memory_usage(train)\n    reduce_memory_usage(test)\n        \ndef do_fe_enc(train, test,cols):\n    print('do_fe_enc')\n    for c in cols:\n        tmp = pd.concat([train[[c]], test[[c]]])\n        enc = tmp[c].value_counts().to_dict()   \n        train[c+'_fq_enc'] = train[c].map(enc)\n        test[c+'_fq_enc']  = test[c].map(enc)\n    del tmp, enc\n    gc.collect()\n        \ndef fe_id_33(train, test):\n    print('fe_id_33')\n    train['id_33'] = train['id_33'].fillna('0x0')\n    train['id_33_0'] = train['id_33'].apply(lambda x: x.split('x')[0]).astype(int).replace({0: np.NaN})\n    train['id_33_1'] = train['id_33'].apply(lambda x: x.split('x')[1]).astype(int).replace({0: np.NaN})\n\n    test['id_33'] = test['id_33'].fillna('0x0')\n    test['id_33_0'] = test['id_33'].apply(lambda x: x.split('x')[0]).astype(int).replace({0: np.NaN})\n    test['id_33_1'] = test['id_33'].apply(lambda x: x.split('x')[1]).astype(int).replace({0: np.NaN})\n    \ndef fe_date_agg(train,test):\n    print('fe_date_agg')\n    for date in ['Day','Weekday','Hour']:\n        mean_ = (train.groupby(date)['TransactionAmt'].transform('mean') + test.groupby(date)['TransactionAmt'].transform('mean'))/2\n        train['TransactionAmt_by_'+date] = train['TransactionAmt'] / mean_\n        test['TransactionAmt_by_'+date] = test['TransactionAmt'] / mean_\n    train['is_holiday'] = train.Weekday.apply(lambda x: 1 if x in [5,6] else 0)\n    test['is_holiday'] = test.Weekday.apply(lambda x: 1 if x in [5,6] else 0)\n    \n    mean_ = (train.groupby('Month')['TransactionAmt'].transform('count').mean() + test.groupby('Month')['TransactionAmt'].transform('count').mean())/2\n    train['Month_rate'] = train.groupby('Month')['TransactionAmt'].transform('count') / mean_\n    test['Month_rate'] = test.groupby('Month')['TransactionAmt'].transform('count') / mean_\n    \n    mean_ = (train.groupby('Day')['TransactionAmt'].transform('count').mean()+test.groupby('Day')['TransactionAmt'].transform('count').mean())/2\n    train['Day_rate'] = train.groupby('Day')['TransactionAmt'].transform('count') / mean_\n    test['Day_rate'] = test.groupby('Day')['TransactionAmt'].transform('count') / mean_\n    \n    mean_ = (train.groupby('Weekday')['TransactionAmt'].transform('count').mean()+test.groupby('Weekday')['TransactionAmt'].transform('count').mean())/2\n    train['Weekday_rate'] = train.groupby('Weekday')['TransactionAmt'].transform('count') / mean_\n    test['Weekday_rate'] = test.groupby('Weekday')['TransactionAmt'].transform('count') / mean_\n    train.drop(['Date','Month','Day','Hour','Weekday','DayOfYear'],1,inplace=True)\n    test.drop(['Date','Month','Day','Hour','Weekday','DayOfYear'],1,inplace=True)\n    \n\ndef fe_dec(train, test):\n    print('fe_dec')\n    def change(hoge):\n        num = 3\n        hoge = int(hoge*1000)\n        while(hoge % 10 ==0):\n            num = num-1\n            hoge = hoge /10\n        if num<0:\n            num = 0\n        return num\n\n    train[\"TransactionAmt_decimal\"] = train[\"TransactionAmt\"].map(change)\n    test[\"TransactionAmt_decimal\"] = test[\"TransactionAmt\"].map(change)\n    \ndef tr_bin(train, test):\n    train.TransactionAmt = np.log(train.TransactionAmt)\n    test.TransactionAmt = np.log(test.TransactionAmt)\n    q, bins = pd.qcut(train.TransactionAmt, 5, retbins=True)\n    train.TransactionAmt = q\n    test.TransactionAmt = pd.cut(test.TransactionAmt, bins)\n    \ndef fe_nan(train, test):\n    print('fe_nan')\n    cols = list(train.columns)\n    cols.remove('isFraud')\n    cols.remove('TransactionDT')\n    cols.remove('TransactionID')\n    for c in cols:\n        train[c+'_nan'] = np.where(train[c].isna(),0,1)\n        test[c+'_nan'] = np.where(test[c].isna(),0,1)\n        ","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# pseudo + influence"},{"metadata":{"trusted":true},"cell_type":"code","source":"def get_pseudo(train,test, preds):\n    test['target'] = preds\n    data = test[(test['target']<=0.01) | (test2['target']>=0.99) ].copy()\n    print('Added ',data.shape[0], 'new instances.')\n    data.loc[data['target']>=0.5, 'target' ] = 1\n    data.loc[data['target']<0.5, 'target' ] = 0\n    train = pd.concat([train,data],axis=0)\n    train.reset_index(drop=True,inplace=True)\n    test.drop('target',1,inplace=True)\n    return train, test\n\ndef get_weights(X,y, X_pseudo):\n    imps = catboost.CatBoostClassifier().get_object_importance(catboost.Pool(data=X_pseudo.drop('target'), label=X_pseudo.target),\n                                                               catboost.Pool(data=X, label=y))\n    return imps\n\ndef do_augment(x, y, t=2):\n    xp, xn = [], []\n    for i in range(t):\n        mask = y > 0\n        x1 = x[mask].copy()\n        ids = np.arange(x1.shape[0])\n        for c in range(x1.shape[1]):\n            np.random.shuffle(ids)\n            x1[:, c] = x1[ids][:, c]\n        xp.append(x1)\n\n    for i in range(t // 2):\n        mask = y == 0\n        x1 = x[mask].copy()\n        ids = np.arange(x1.shape[0])\n        for c in range(x1.shape[1]):\n            np.random.shuffle(ids)\n            x1[:, c] = x1[ids][:, c]\n        xn.append(x1)\n\n    xp = np.vstack(xp)\n    xn = np.vstack(xn)\n    yp = np.ones(xp.shape[0])\n    yn = np.zeros(xn.shape[0])\n    x = np.vstack([x, xp, xn])\n    y = np.concatenate([y, yp, yn])\n    return x, y","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# interactions"},{"metadata":{"trusted":true},"cell_type":"code","source":"pairs_cols=[['C1', 'V258'],\n ['C1', 'C14'],\n ['C1', 'C13'],\n ['C4', 'V258'],\n ['C14', 'V294'],\n ['C7', 'V258'],\n ['V258', 'V294'],\n ['V294', 'card6'],\n ['C11', 'C13'],\n ['C14', 'C8'],\n ['C11', 'C14'],\n ['C11', 'V70'],\n ['C8', 'V258'],\n ['C11', 'D2'],\n ['C14', 'C4'],\n ['C14', 'D2'],\n ['C1', 'V189'],\n ['V312', 'V70'],\n ['V189', 'V258'],\n ['C14', 'V317'],\n ['C14', 'V308'],\n ['C13', 'D2'],\n ['C4', 'D2'],\n ['V70', 'card6'],\n ['C1', 'C4'],\n ['V189', 'V45'],\n ['C12', 'C13'],\n ['V91', 'card6'],\n ['C8', 'D2'],\n ['C1', 'V201'],\n ['C12', 'V258'],\n ['C1', 'C5'],\n ['C13', 'C7'],\n ['C11', 'V258'],\n ['V201', 'V294'],\n ['C14', 'V283'],\n ['C13', 'V45'],\n ['C5', 'V312'],\n ['C14', 'id_17'],\n ['C13', 'C8'],\n ['C11', 'V91'],\n ['C13', 'C13'],\n ['D2', 'V308'],\n ['V149', 'card6'],\n ['card2', 'id_01'],\n ['V283', 'card6'],\n ['C7', 'V156'],\n ['C1', 'R_emaildomain'],\n ['V294', 'V62'],\n ['V317', 'card6'],\n ['V258', 'V45'],\n ['C14', 'V189'],\n ['V294', 'V45'],\n ['C13', 'C4'],\n ['D2', 'TransactionAmt'],\n ['C14', 'V156'],\n ['V62', 'card6'],\n ['ProductCD', 'TransactionAmt'],\n ['C11', 'C5'],\n ['V187', 'V258'],\n ['C14', 'addr2'],\n ['C14', 'V312'],\n ['TransactionAmt', 'card2'],\n ['V129', 'V70'],\n ['V283', 'V308'],\n ['TransactionAmt', 'card3'],\n ['C1', 'id_17'],\n ['C14', 'R_emaildomain'],\n ['C1', 'D2'],\n ['C4', 'V294'],\n ['V244', 'V45'],\n ['V258', 'V258'],\n ['C7', 'V149'],\n ['D3', 'V294'],\n ['D10', 'card6'],\n ['V149', 'id_30'],\n ['M5', 'V45'],\n ['D2', 'V62'],\n ['C1', 'V169'],\n ['V189', 'V294'],\n ['C12', 'V294'],\n ['C1', 'C11'],\n ['C4', 'V317'],\n ['D2', 'V83'],\n ['C1', 'V223'],\n ['D3', 'card6'],\n ['C14', 'id_01'],\n ['C8', 'V317'],\n ['C14', 'V333'],\n ['C14', 'card3'],\n ['V129', 'V91'],\n ['C1', 'V244'],\n ['V244', 'V294'],\n ['C13', 'V317'],\n ['V128', 'V91'],\n ['V12', 'V317'],\n ['V149', 'V156'],\n ['V243', 'V317'],\n ['V223', 'V244'],\n ['C1', 'id_13']]\n\ndef fe_pairs(train, test):\n    print('fe_pairs')\n    inter_cols = []\n    def do_pair(c1,c2):\n        print(c1,c2)\n        train[c1+'_'+c2] = train[c1] * train[c2]\n        test[c1+'_'+c2] = test[c1] * test[c2]\n        return c1+'_'+c2\n    for c in pairs_cols:\n        inter_cols.append(do_pair(c[0],c[1]))\n    return inter_cols","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# under, over sampling"},{"metadata":{"trusted":true},"cell_type":"code","source":"# TransactionAmt, dist - RobustScaler, np.log\n# card, addr - categories","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"gc.collect()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train, test,sub = load_data()\ntrain.drop('id_31',1,inplace=True)\ntest.drop('id_31',1,inplace=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# fe_nan(train, test)\nfe_id_30(train,test)\nfe_id_33(train,test)\nfe_mail(train,test)\nfe_dec(train, test)\n\nfe_date(train,test)\nfe_date_agg(train,test)\ntr_bin(train, test)\n\nobj_cols = list(train.select_dtypes('object').columns)\nnum_cols = [c for c in train.columns if c not in obj_cols]\n\nfe_cat_other(train,test,obj_cols)\nobj_cols.append('TransactionAmt')\nfe_le(train,test, obj_cols)\npairs_cols = fe_pairs(train, test)\n# fe_smooth(train,test)\ndo_fe_enc(train, test, obj_cols)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X = train.drop(['isFraud',  'TransactionID','TransactionDT'], axis=1)\ny = train['isFraud']\ndel train\ngc.collect()\n\nX_test = test.drop(['TransactionID','TransactionDT'], axis=1)\ntest = test[[\"TransactionDT\", 'TransactionID']]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# adv(X,X_test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# TRAIN AUC: 0.9497488486026882\t TRAIN LOG: 0.06640471851123202\n# VALID AUC: 0.9269320753003503\t VALID LOG: 0.08852609545761803 base\n\n# TRAIN AUC: 0.9509330521028726\t TRAIN LOG: 0.06577636329065795\n# VALID AUC: 0.9241533723666887\t VALID LOG: 0.0894685065111738","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X_train = X.head(550000)\nY_train = y[X_train.index]\nX_valid = X.tail(40540)\nY_valid = y[X_valid.index]\ndel X\ngc.collect()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"dtrain = lgb.Dataset(data=X_train, label=Y_train, free_raw_data=False)\ndel X_train, Y_train\ngc.collect()\ndval = lgb.Dataset(data=X_valid, label=Y_valid, free_raw_data=False)\ndel X_valid, Y_valid\ngc.collect()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"clf = lgb.train(lgb_params, dtrain, \n                num_boost_round=1000,\n                verbose_eval=200,\n                early_stopping_rounds=100, \n                valid_sets = [dtrain, dval])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"feature_importance = pd.DataFrame()\nfeature_importance[\"feature\"] = X_test.columns\nfeature_importance[\"importance\"] = clf.feature_importance()\ncols = feature_importance[[\"feature\", \"importance\"]].groupby(\"feature\").mean().sort_values(\"importance\", ascending=False)[:50].index\nbest_features = feature_importance.loc[feature_importance.feature.isin(cols)]\nplt.figure(figsize=(16, 12))\nsns.barplot(x=\"importance\",\n                y=\"feature\",\n                data=best_features.sort_values(by=\"importance\", ascending=False))\nplt.title('LGB Features (avg over folds)')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X_train = dtrain.get_data()\nY_train = dtrain.get_label()\ndel dtrain\ngc.collect()\n\nX_valid = dval.get_data()\nY_valid = dval.get_label()\ndel dval\ngc.collect()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"oof = clf.predict(X_valid)\ntrain_pred = clf.predict(X_train)\nprint(f'TRAIN AUC: {roc_auc_score(Y_train, train_pred)}\\t TRAIN LOG: {log_loss(Y_train,  train_pred)}')\nprint(f'VALID AUC: {roc_auc_score(Y_valid, oof)}\\t VALID LOG: {log_loss(Y_valid,  oof)}')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X = pd.concat([X_train,X_valid])\ndel X_train, X_valid\ngc.collect()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"shap.initjs()\nexplainer = shap.TreeExplainer(clf)\nvalues = explainer.shap_values(X.tail(40000))\nshap.summary_plot(values, X.tail(40000), plot_type=\"bar\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# clf,num_iter =eval_ts(X,y)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"dtrain=lgb.Dataset(data=X, label=y)\ndel X\ngc.collect()\nnum_iter = clf.best_iteration\nclf = lgb.train(lgb_params, dtrain, \n            num_boost_round=num_iter,\n            verbose_eval=200)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test = test.sort_values('TransactionDT')\ntest['prediction'] = clf.predict(X_test)\nsub['isFraud'] = pd.merge(sub, test, on='TransactionID')['prediction']\nsub.to_csv('rus.csv', index=False)\nsub.tail()","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.4","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}