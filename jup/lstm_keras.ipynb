{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"import numpy as np\nimport pandas as pd\n\nimport gc, pickle, random, os, operator\nfrom tqdm import tqdm\n\nfrom keras.models import Model\nfrom keras.layers import Input, Dense, Embedding, SpatialDropout1D, add, concatenate\nfrom keras.layers import CuDNNLSTM, Bidirectional, GlobalMaxPooling1D, GlobalAveragePooling1D\nfrom keras.preprocessing import text, sequence\n\n# Opens embeddings from gensim\nfrom gensim.models import KeyedVectors","execution_count":1,"outputs":[{"output_type":"stream","text":"Using TensorFlow backend.\n","name":"stderr"}]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"EMBEDDING_FILES = [\n#     '../input/gensim-embeddings-dataset/crawl-300d-2M.gensim',\n#     '../input/gensim-embeddings-dataset/glove.840B.300d.gensim',\n    '../input/pickled-crawl300d2m-for-kernel-competitions/crawl-300d-2M.pkl',\n    '../input/pickled-glove840b300d-for-10sec-loading/glove.840B.300d.pkl'\n]\nSEED = 7321\nNUM_MODELS = 2\nBATCH_SIZE = 512\nLSTM_UNITS = 128\nDENSE_HIDDEN_UNITS = 4 * LSTM_UNITS\nEPOCHS = 4\nMAX_LEN = 220\n\nIDENTITY_COLUMNS = [\n    'male', 'female', 'homosexual_gay_or_lesbian', 'christian', 'jewish',\n    'muslim', 'black', 'white', 'psychiatric_or_mental_illness'\n]\nAUX_COLUMNS = ['target', 'severe_toxicity', 'obscene', 'identity_attack', 'insult', 'threat']\nTEXT_COLUMN = 'comment_text'\nTARGET_COLUMN = 'target'\n\nCHARS_TO_REMOVE = '!\"#$%&()*+,-./:;<=>?@[\\\\]^_`{|}~\\t\\n“”’\\'∞θ÷α•à−β∅³π‘₹´°£€\\×™√²—'","execution_count":2,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def load_embeddings(path):\n    with open(path,'rb') as f:\n        emb_arr = pickle.load(f)\n    return emb_arr\n\ndef set_seed(seed):\n    random.seed(seed)\n    os.environ['PYTHONHASHSEED'] = str(seed)\n    np.random.seed(seed)\nset_seed(SEED)","execution_count":3,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def build_vocab(sentences, verbose =  True):\n    vocab = {}\n    for sentence in tqdm(sentences, disable = (not verbose)):\n        for word in sentence:\n            try:\n                vocab[word] += 1\n            except KeyError:\n                vocab[word] = 1\n    return vocab","execution_count":4,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def build_matrix(word_index, path, return_oov = False):\n    embedding_index = load_embeddings(path)\n    embedding_matrix = np.zeros((len(word_index) + 1, 300))\n    oov = []\n    \n    for word, i in word_index.items():\n        found = False\n        for candidate in [word, word.lower(), word.title()]:\n            if candidate in embedding_index:\n                found = True\n                embedding_matrix[i] = embedding_index[candidate]\n                break\n        if not found:\n            oov.append(word)\n    \n    tkz_len = len(word_index)\n    emb_len = tkz_len - len(oov)\n    print('{} found embeddings. {:.2%} of total.'.format(emb_len, emb_len/N_WORDS))\n    if return_oov:\n        return embedding_matrix, unknown_words\n    else:\n        return embedding_matrix","execution_count":21,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def build_model(embedding_matrix, num_aux_targets):\n    words = Input(shape=(None,))\n    x = Embedding(*embedding_matrix.shape, weights=[embedding_matrix], trainable=False)(words)\n    x = SpatialDropout1D(0.2)(x)\n    x = Bidirectional(CuDNNLSTM(LSTM_UNITS, return_sequences=True))(x)\n    x = Bidirectional(CuDNNLSTM(LSTM_UNITS, return_sequences=True))(x)\n\n    hidden = concatenate([\n        GlobalMaxPooling1D()(x),\n        GlobalAveragePooling1D()(x),\n    ])\n    hidden = add([hidden, Dense(DENSE_HIDDEN_UNITS, activation='relu')(hidden)])\n    hidden = add([hidden, Dense(DENSE_HIDDEN_UNITS, activation='relu')(hidden)])\n    result = Dense(1, activation='sigmoid')(hidden)\n    aux_result = Dense(num_aux_targets, activation='sigmoid')(hidden)\n    \n    model = Model(inputs=words, outputs=[result, aux_result])\n    model.compile(loss='binary_crossentropy', optimizer='adam')\n\n    return model","execution_count":6,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"path = '../input/jigsaw-unintended-bias-in-toxicity-classification/'\nnrows = 2**15\ntrain = pd.read_csv(path + 'train.csv', nrows=nrows)\ntest = pd.read_csv(path + 'test.csv', nrows=nrows)","execution_count":12,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"vocab = build_vocab(list(train[TEXT_COLUMN].apply(lambda x: x.split())))\nN_WORDS = len(vocab)\ndel vocab\ngc.collect()","execution_count":13,"outputs":[{"output_type":"stream","text":"100%|██████████| 32768/32768 [00:00<00:00, 57348.67it/s]\n","name":"stderr"},{"output_type":"execute_result","execution_count":13,"data":{"text/plain":"11"},"metadata":{}}]},{"metadata":{"trusted":true},"cell_type":"code","source":"X_train = train[TEXT_COLUMN].astype(str)\ny_train = train[TARGET_COLUMN].values\ny_aux_train = train[AUX_COLUMNS].values\nX_test = test[TEXT_COLUMN].astype(str)\ntrain = train[IDENTITY_COLUMNS + [TARGET_COLUMN]]\ntest = test[['id']]","execution_count":14,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"for c in IDENTITY_COLUMNS + [TARGET_COLUMN]:\n    train[c] = np.where(train[c] >= 0.5, True, False)","execution_count":15,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"tokenizer  = text.Tokenizer(filters=CHARS_TO_REMOVE, lower=False)\ntokenizer.fit_on_texts(list(X_train) + list(X_test))","execution_count":16,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X_train = tokenizer.texts_to_sequences(X_train)\nX_test = tokenizer.texts_to_sequences(X_test)\nX_train = sequence.pad_sequences(X_train, maxlen=MAX_LEN)\nX_test = sequence.pad_sequences(X_test, maxlen=MAX_LEN)","execution_count":17,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sample_weights = np.ones(len(X_train), dtype=np.float32)\nsample_weights += train[IDENTITY_COLUMNS].sum(axis=1)\nsample_weights += train[TARGET_COLUMN] * (~train[IDENTITY_COLUMNS]).sum(axis=1)\nsample_weights += (~train[TARGET_COLUMN]) * train[IDENTITY_COLUMNS].sum(axis=1) * 5\nsample_weights /= sample_weights.mean()","execution_count":18,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(f'{N_WORDS} unique words.')\ntkz_len = len(tokenizer.word_index)\nprint('{} words in tokenizer. {:.2%} of total.'.format(tkz_len, tkz_len/N_WORDS))","execution_count":24,"outputs":[{"output_type":"stream","text":"120173 unique words.\n78860 words in tokenizer. 65.62% of total.\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"embedding_matrix = np.concatenate(\n    [build_matrix(tokenizer.word_index, f, return_oov=False) for f in EMBEDDING_FILES], axis=-1)\noof = []\nweights = []","execution_count":25,"outputs":[{"output_type":"stream","text":"71431 founded embeddings. 59.44% of total.\n71235 founded embeddings. 59.28% of total.\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"for model_n in range(NUM_MODELS):\n    model = build_model(embedding_matrix, y_aux_train.shape[-1])\n    for global_epoch in range(EPOCHS):\n        model.fit(\n            X_train, [y_train, y_aux_train],\n            batch_size=BATCH_SIZE,\n            epochs=1,\n            verbose=2,\n            sample_weight = [sample_weights.values, np.ones_like(sample_weights)]\n        )\n        oof.append(model.predict(X_test, batch_size=2048)[0].flatten())\n        weights.append(2 ** global_epoch)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# --------------------------------\n# No emb. cov. increasing, emb. concat (300+300), static pad to 220 on whole dataset\n# 120173 unique words.\n# 78860 words in tokenizer. 65.62% of total.\n# 71431 founded embeddings. 59.44% of total.\n# 71235 founded embeddings. 59.28% of total.\n# - 14s - loss: 0.4859 - dense_7_loss: 0.3882 - dense_8_loss: 0.0977\n# --------------------------------\n# ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"preds = np.average(oof, weights=weights, axis=0)\nsub = pd.DataFrame.from_dict({\n    'id': test.id,\n    'prediction': preds\n})\nsub.to_csv('sub.csv', index = False)\nsub.tail()","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":1}